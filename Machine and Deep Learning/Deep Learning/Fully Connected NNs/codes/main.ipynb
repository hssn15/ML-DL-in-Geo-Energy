{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p-ILzMJhvIN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from Util1 import DataManipulation\n",
        "\n",
        "from joblib import dump, load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfW-jqPOhvIP"
      },
      "outputs": [],
      "source": [
        "# device selection\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_VCNw-6hvIQ"
      },
      "source": [
        "# 1. DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP3mDNHGhvIR"
      },
      "outputs": [],
      "source": [
        "Data = pd.read_excel(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\data\\Geophysical_Logs_Well_1.xlsx', sheet_name='Well_1')\n",
        "Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTxbkfUChvIR"
      },
      "source": [
        "## a) Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1UJXzKXhvIR"
      },
      "outputs": [],
      "source": [
        "# missing data visualziation\n",
        "m = DataManipulation(Data)\n",
        "m.VisualizeMissingData()\n",
        "m.MissingDataSummarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3pfa14shvIS"
      },
      "outputs": [],
      "source": [
        "# removing missing data rows\n",
        "Data_Manipulated = m.DropMissingData()\n",
        "m.VisualizeMissingData()\n",
        "m.MissingDataSummarizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Myf51PQhvIS"
      },
      "source": [
        "## b) Remove Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSDlSGifhvIS"
      },
      "outputs": [],
      "source": [
        "# visualization of Cross and Box Plots to dedect outliers\n",
        "m.CrossPlot()\n",
        "m.BoxPlot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMG2IY2EhvIT"
      },
      "outputs": [],
      "source": [
        "# Dedecting Outliers using Tukey's Method\n",
        "for i in np.array(Data_Manipulated.columns):\n",
        "    if not i in ['DEPTH','Medium Resistivity (RM)', 'Density (RHOB)']:\n",
        "        data_tukey = Data_Manipulated[i]\n",
        "        outliers = m.tukey_outliers(data_tukey, Data_Manipulated.index)[0]\n",
        "        print(i, outliers)\n",
        "        plt.hist(data_tukey, bins=50, color='gray', alpha=0.5)\n",
        "        plt.xlabel(i)\n",
        "        plt.ylabel('N')\n",
        "        plt.title(\"Outlier Detection\")\n",
        "        plt.scatter(outliers, [0 for j in range(len(outliers))], color='red')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO0pUFJIhvIT"
      },
      "source": [
        "Based on  the results from Tukey's outlier detection, we can be sure that the numbers greater than or equal to 999 are outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYZq1ZCphvIT"
      },
      "outputs": [],
      "source": [
        "for i in np.array(Data_Manipulated.columns):\n",
        "    if not i in ['DEPTH','Medium Resistivity (RM)', 'Density (RHOB)']:\n",
        "        Data_Manipulated = Data_Manipulated[Data_Manipulated[i]<999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0f9_yI2hvIT"
      },
      "outputs": [],
      "source": [
        "# After removing outliers\n",
        "m = DataManipulation(Data_Manipulated)\n",
        "m.CrossPlot()\n",
        "m.BoxPlot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9mceJVqhvIT"
      },
      "outputs": [],
      "source": [
        "m.MissingDataSummarizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D07hVwdYhvIT"
      },
      "source": [
        "## c) Scaling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dstrhUmBhvIT"
      },
      "outputs": [],
      "source": [
        "X = Data_Manipulated.drop([\"Permeability (Perm)\", \"Water Saturation (SW)\"], axis = 1)\n",
        "y = Data_Manipulated[[\"Permeability (Perm)\", \"Water Saturation (SW)\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgDMOeEAhvIU"
      },
      "outputs": [],
      "source": [
        "# Data scaling\n",
        "#X_scaler=StandardScaler()\n",
        "#y_scaler=StandardScaler()\n",
        "#\n",
        "#X_scaled = X_scaler.fit(X)\n",
        "#y_scaled = y_scaler.fit(y)\n",
        "#\n",
        "#dump(X_scaler, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\saved models\\X_scaler.joblib')\n",
        "#dump(y_scaler, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\saved models\\y_scaler.joblib')\n",
        "\n",
        "# load the saved model from a file\n",
        "X_scaler = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\saved models\\X_scaler.joblib')\n",
        "y_scaler = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\saved models\\y_scaler.joblib')\n",
        "\n",
        "X_scaled = X_scaler.transform(X)\n",
        "y_scaled = y_scaler.transform(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzCzbSJrhvIU"
      },
      "source": [
        "## d) Split the Dataset into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqZWsm54hvIU"
      },
      "outputs": [],
      "source": [
        "X_scaled_train, X_scaled_test, y_scaled_train, y_scaled_test = train_test_split(X_scaled, y_scaled, test_size= 0.33, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOGRcKJuhvIU"
      },
      "outputs": [],
      "source": [
        "X_scaled_train = torch.tensor(X_scaled_train, dtype=torch.float).to(device = device)\n",
        "X_scaled_test  = torch.tensor(X_scaled_test , dtype=torch.float).to(device = device)\n",
        "y_scaled_train = torch.tensor(y_scaled_train, dtype=torch.float).to(device = device)\n",
        "y_scaled_test  = torch.tensor(y_scaled_test , dtype=torch.float).to(device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDcj9IDmhvIU"
      },
      "source": [
        "# 2. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK1guM1khvIU"
      },
      "outputs": [],
      "source": [
        "class NN_model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.activation = torch.nn.Tanh()\n",
        "        self.loss_function = torch.nn.MSELoss(reduction ='mean')\n",
        "        self.linears = torch.nn.ModuleList([torch.nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
        "\n",
        "        for i in range(len(layers)-1):\n",
        "            torch.nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
        "            torch.nn.init.zeros_(self.linears[i].bias.data)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if torch.is_tensor(x) != True:\n",
        "            x = torch.from_numpy(x)\n",
        "        a = x.float()\n",
        "        for i in range(len(self.layers)-2):\n",
        "            z = self.linears[i](a)\n",
        "            a = self.activation(z)\n",
        "        a = self.linears[-1](a)\n",
        "        return a\n",
        "\n",
        "    def loss(self, x, y_target ):\n",
        "        L = self.loss_function(self.forward(x), y_target)\n",
        "        return L\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        y_pred = self.forward(x_test)\n",
        "        y_pred = y_pred.cpu().detach().numpy()\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SyVeo1ShvIU"
      },
      "outputs": [],
      "source": [
        "input_dimension = X_scaled.shape[1]\n",
        "output_dimension = y_scaled.shape[1]\n",
        "layer_architectures = [[input_dimension, 10, 10, 10, output_dimension], [input_dimension, 15, 15, 15,15, output_dimension], [input_dimension, 20, 20, 20, output_dimension], [input_dimension, 25, 25, 25,25, output_dimension], [input_dimension, 30, 30, 30, output_dimension]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnE-eXMhhvIU"
      },
      "outputs": [],
      "source": [
        "def R2(y_train_true, y_train_pred, y_test_true, y_test_pred, target):\n",
        "    \"\"\"R2 plot calculation\"\"\"\n",
        "    # Get prediciton score in terms of R2\n",
        "    r2_test = r2_score(y_test_true, y_test_pred)\n",
        "    r2_train = r2_score(y_train_true, y_train_pred)\n",
        "\n",
        "    # Plot parity plots\n",
        "    fs = 9\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.suptitle('${}$ Parity Plot'.format(target), fontsize=fs)\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(y_test_true, y_test_pred, 'ro', label=\"Test: R2 = {}\".format(round(r2_test, 3)))\n",
        "    plt.plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'k-.')\n",
        "    plt.ylabel('Prediction, {}'.format(target), fontsize=fs)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(y_train_true, y_train_pred, 'bo', label=\"Train: R2 = {}\".format(round(r2_train, 3)))\n",
        "    plt.plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'k-.')\n",
        "    plt.ylabel('Prediction, {}'.format(target), fontsize=fs)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.xlabel('True, {}'.format(target), fontsize=fs)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGOj6uqhvIU"
      },
      "source": [
        "# 3) Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06VH43C_hvIU"
      },
      "outputs": [],
      "source": [
        "# Adam Optimizer Parameters\n",
        "learing_rate_Adam = [0.5e-3, 1e-3, 1.5e-3]\n",
        "N_epochs_lst_Adam = [1000, 2000,  4000]\n",
        "eps= 1e-08\n",
        "weight_decay=0\n",
        "amsgrad_lst = [False, True]\n",
        "\n",
        "# LBFGS Oprimizer Parameters\n",
        "learing_rate_LBFGS = [1.0, 1.5, 2.0]\n",
        "N_epochs_lst_LBFGS = [1, 3, 5]\n",
        "max_iter_lst=[100, 1000, 10000]\n",
        "history_size_lst = [5, 10, 15]\n",
        "line_search_fn = 'strong_wolfe'\n",
        "tolerance_grad = 1e-07\n",
        "tolerance_change = 1e-09"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeQK4dK0hvIV"
      },
      "outputs": [],
      "source": [
        "def training_w_Adam(model, x_test, x_train, y_test_target, y_train_target, lr, N_epochs, eps, weight_decay, amsgrad, n, save_model):\n",
        "    training_loss_list = []\n",
        "    test_loss_list = []\n",
        "    # Setting Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "    # Training Loop\n",
        "    for i in range(N_epochs):\n",
        "        # initialize optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        train_loss = model.loss(x_train, y_train_target)\n",
        "        test_loss = model.loss(x_test, y_test_target)\n",
        "         # collecting loss\n",
        "        test_loss_list.append(test_loss.item())\n",
        "        training_loss_list.append(train_loss.item())\n",
        "        # print loss values\n",
        "        print(\"# Epoch  = {} \".format(i), \" | Train Loss = {}\".format(train_loss.item()), \" | Test Loss = {}\".format(test_loss.item()))\n",
        "        # backward pass\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "    if save_model == True:\n",
        "        torch.save(model.state_dict(), r\"C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\adam saved nn models\\adam_model_{}.pth\".format(n))\n",
        "    return training_loss_list, test_loss_list\n",
        "\n",
        "def training_w_LBFGS(model, x_test, x_train, y_test_target, y_train_target, lr, N_epochs, max_iter, history_size, tolerance_grad, tolerance_change, line_search_fn, n, save_model):\n",
        "    training_loss_list = []\n",
        "    test_loss_list = []\n",
        "    # Setting Optimizer\n",
        "    optimizer = torch.optim.LBFGS(model.parameters(),lr=lr, max_iter=max_iter, history_size = history_size, tolerance_grad=tolerance_grad, tolerance_change = tolerance_change, line_search_fn = line_search_fn)\n",
        "    # Training Loop\n",
        "    for i in range(N_epochs):\n",
        "        def closure():\n",
        "            # initialize optimizer\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            train_loss = model.loss(x_train, y_train_target)\n",
        "            test_loss = model.loss(x_test, y_test_target)\n",
        "            # collecting loss\n",
        "            training_loss_list.append(train_loss.item())\n",
        "            test_loss_list.append(test_loss.item())\n",
        "            print(\" | Train Loss = {}\".format(train_loss.item()), \" | Test Loss = {}\".format(test_loss.item()))\n",
        "            # backward pass\n",
        "            train_loss.backward()\n",
        "            return train_loss\n",
        "        optimizer.step(closure)\n",
        "    if save_model == True:\n",
        "        torch.save(model.state_dict(), r\"C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\lbfgs saved nn models\\lbfgs_model_{}.pth\".format(n))\n",
        "    return training_loss_list, test_loss_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcyQHEbShvIV"
      },
      "source": [
        "## Model Using Adam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRLzRytIhvIV"
      },
      "outputs": [],
      "source": [
        "# Training and Testing By choosing random parameters: Adam\n",
        "layers = [input_dimension, 15, 15, 15, 15, 15, output_dimension]\n",
        "model = NN_model(layers).to(device=device)\n",
        "lr = 1e-3\n",
        "N_epochs = 10000\n",
        "amsgrad = True\n",
        "n = 0\n",
        "train_loss, test_loss = training_w_Adam(model, X_scaled_test, X_scaled_train, y_scaled_test, y_scaled_train, lr, N_epochs, eps, weight_decay, amsgrad, n, save_model= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxq-mIu-hvIV"
      },
      "outputs": [],
      "source": [
        "y_scaled_test_pred = model.predict(X_scaled_test)\n",
        "y_test_pred = y_scaler.inverse_transform(y_scaled_test_pred)\n",
        "y_test_true = y_scaler.inverse_transform(y_scaled_test.cpu().numpy())\n",
        "\n",
        "y_scaled_train_pred = model.predict(X_scaled_train)\n",
        "y_train_pred = y_scaler.inverse_transform(y_scaled_train_pred)\n",
        "y_train_true = y_scaler.inverse_transform(y_scaled_train.cpu().numpy())\n",
        "\n",
        "R2(y_train_true[:, 0], y_train_pred[:, 0], y_test_true[:, 0], y_test_pred[:, 0], \"Permeability\")\n",
        "R2(y_train_true[:, 1], y_train_pred[:, 1], y_test_true[:, 1], y_test_pred[:, 1], \"Saturation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqmHhCdMhvIV"
      },
      "source": [
        "## Model Using LBFGS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY_QTH3GhvIV"
      },
      "outputs": [],
      "source": [
        "# Training and Testing By choosing random parameters: Adam\n",
        "layers = [input_dimension, 15, 15, 15, 15, 15, output_dimension]\n",
        "model1 = NN_model(layers).to(device=device)\n",
        "lr = 1.0\n",
        "N_epochs = 1\n",
        "max_iter = 2000\n",
        "history_size = 10\n",
        "line_search_fn = 'strong_wolfe'\n",
        "tolerance_grad = 1e-07\n",
        "tolerance_change = 1e-09\n",
        "train_loss, test_loss = training_w_LBFGS(model1, X_scaled_test, X_scaled_train, y_scaled_test, y_scaled_train, lr, N_epochs, max_iter, history_size, tolerance_grad, tolerance_change, line_search_fn, n, save_model = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x8jhWBMhvIW"
      },
      "outputs": [],
      "source": [
        "y_scaled_test_pred = model1.predict(X_scaled_test)\n",
        "y_test_pred = y_scaler.inverse_transform(y_scaled_test_pred)\n",
        "y_test_true = y_scaler.inverse_transform(y_scaled_test.cpu().numpy())\n",
        "\n",
        "y_scaled_train_pred = model1.predict(X_scaled_train)\n",
        "y_train_pred = y_scaler.inverse_transform(y_scaled_train_pred)\n",
        "y_train_true = y_scaler.inverse_transform(y_scaled_train.cpu().numpy())\n",
        "\n",
        "R2(y_train_true[:, 0], y_train_pred[:, 0], y_test_true[:, 0], y_test_pred[:, 0], \"Permeability\")\n",
        "R2(y_train_true[:, 1], y_train_pred[:, 1], y_test_true[:, 1], y_test_pred[:, 1], \"Saturation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qskiAcjBhvIW"
      },
      "source": [
        "# FINE-TUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdbdQUfRhvIW"
      },
      "source": [
        "## Fine - Tuning : Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-AeBnPbhvIW"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "adam_model_parameters = {}\n",
        "adam_train_test_losses = {}\n",
        "for layers in  layer_architectures:\n",
        "    for lr in learing_rate_Adam:\n",
        "        for N_epochs in N_epochs_lst_Adam:\n",
        "            for amsgrad in  amsgrad_lst:\n",
        "                print(\"Model #{} is started\".format(n))\n",
        "                aux = []\n",
        "                aux.append([layers, lr, N_epochs, amsgrad])\n",
        "                adam_model_parameters[n] = aux\n",
        "                model = NN_model(layers).to(device=device)\n",
        "                adam_train_test_losses[n] = training_w_Adam(model, X_scaled_test, X_scaled_train, y_scaled_test, y_scaled_train, lr, N_epochs, eps, weight_decay, amsgrad, n, save_model= True)\n",
        "                n+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85nKubXRhvIW"
      },
      "source": [
        "## Evaluation of Model: Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpqWKY_WhvIW"
      },
      "outputs": [],
      "source": [
        "# Selecting Best Model and Plot Loss - Train\n",
        "best_model = 0\n",
        "best_model_loss = 1000\n",
        "for i in adam_train_test_losses.keys():\n",
        "    if adam_train_test_losses[i][1][-1] < best_model_loss:\n",
        "        best_model = i\n",
        "        best_model_loss = adam_train_test_losses[i][1][-1]\n",
        "\n",
        "plt.title(\"Best Model: Adam\")\n",
        "plt.plot(adam_train_test_losses[best_model][0], color= 'r', label = \"Train Loss\")\n",
        "plt.plot(adam_train_test_losses[best_model][1], color= 'g', label = \"Test Loss\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Predict Using Best Model\n",
        "layers = adam_model_parameters[best_model][0][0]\n",
        "bst_model = NN_model(layers).to(device=device)\n",
        "# Load The Best Model\n",
        "bst_model.load_state_dict(torch.load(r\"C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\adam saved nn models\\adam_model_{}.pth\".format(best_model)))\n",
        "y_scaled_test_pred = bst_model.predict(X_scaled_test)\n",
        "y_test_pred = y_scaler.inverse_transform(y_scaled_test_pred)\n",
        "y_test_true = y_scaler.inverse_transform(y_scaled_test.cpu().numpy())\n",
        "\n",
        "y_scaled_train_pred = bst_model.predict(X_scaled_train)\n",
        "y_train_pred = y_scaler.inverse_transform(y_scaled_train_pred)\n",
        "y_train_true = y_scaler.inverse_transform(y_scaled_train.cpu().numpy())\n",
        "\n",
        "R2(y_train_true[:, 0], y_train_pred[:, 0], y_test_true[:, 0], y_test_pred[:, 0], \"Permeability\")\n",
        "R2(y_train_true[:, 1], y_train_pred[:, 1], y_test_true[:, 1], y_test_pred[:, 1], \"Saturation\")\n",
        "\n",
        "print(\" Adam Best Model Parameters: Model No = {}, Layers = {},  Learning Rate = {}, Number of Epochs = {}, AmsGrad = {}\".format(best_model, adam_model_parameters[best_model][0][0], adam_model_parameters[best_model][0][1], adam_model_parameters[best_model][0][2], adam_model_parameters[best_model][0][3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD3efq2FhvIW"
      },
      "source": [
        "## Fine - Tuning : LBFGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_ZhoRSYhvIX"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "lbfgs_model_parameters = {}\n",
        "lbfgs_train_test_losses = {}\n",
        "max_iter = 4000\n",
        "N_epochs = 1\n",
        "for layers in  layer_architectures:\n",
        "    for lr in learing_rate_LBFGS:\n",
        "        for history_size in  history_size_lst:\n",
        "            print(\"Model #{} is started\".format(n))\n",
        "            aux = []\n",
        "            aux.append([layers, lr, history_size])\n",
        "            lbfgs_model_parameters[n] = aux\n",
        "            model = NN_model(layers).to(device=device)\n",
        "            lbfgs_train_test_losses[n] = training_w_LBFGS(model, X_scaled_test, X_scaled_train, y_scaled_test, y_scaled_train, lr, N_epochs, max_iter, history_size, tolerance_grad, tolerance_change, line_search_fn, n, save_model = True)\n",
        "            n+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTLB71MxhvIX"
      },
      "source": [
        "## Evaluation of Model: Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKVOOvK0hvIY"
      },
      "outputs": [],
      "source": [
        "# Selecting Best Model and Plot Loss - Train\n",
        "best_model_ = 0\n",
        "best_model_loss = 1000\n",
        "for i in lbfgs_train_test_losses.keys():\n",
        "    if np.mean(lbfgs_train_test_losses[i][1]) < best_model_loss:\n",
        "        best_model = i\n",
        "        best_model_loss = np.mean(lbfgs_train_test_losses[i][1])\n",
        "\n",
        "plt.title(\"Best Model: LBFGS\")\n",
        "plt.plot(lbfgs_train_test_losses[best_model][0], color= 'r', label = \"Train Loss\")\n",
        "plt.plot(lbfgs_train_test_losses[best_model][1], color= 'g', label = \"Test Loss\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Predict Using Best Model\n",
        "layers = lbfgs_model_parameters[best_model][0][0]\n",
        "bst_model = NN_model(layers).to(device=device)\n",
        "# Load The Best Model\n",
        "bst_model.load_state_dict(torch.load(r\"C:\\Users\\YUSIFOH\\ERPE 394A\\Huseyn_Yusifov_Homework_4\\lbfgs saved nn models\\lbfgs_model_{}.pth\".format(best_model)))\n",
        "y_scaled_test_pred = bst_model.predict(X_scaled_test)\n",
        "y_test_pred = y_scaler.inverse_transform(y_scaled_test_pred)\n",
        "y_test_true = y_scaler.inverse_transform(y_scaled_test.cpu().numpy())\n",
        "\n",
        "y_scaled_train_pred = bst_model.predict(X_scaled_train)\n",
        "y_train_pred = y_scaler.inverse_transform(y_scaled_train_pred)\n",
        "y_train_true = y_scaler.inverse_transform(y_scaled_train.cpu().numpy())\n",
        "\n",
        "R2(y_train_true[:, 0], y_train_pred[:, 0], y_test_true[:, 0], y_test_pred[:, 0], \"Permeability\")\n",
        "R2(y_train_true[:, 1], y_train_pred[:, 1], y_test_true[:, 1], y_test_pred[:, 1], \"Saturation\")\n",
        "\n",
        "print(\" LBFGS Best Model Parameters: Model No = {}, Layers = {},  Learning Rate = {}, History Size = {}\".format(best_model, lbfgs_model_parameters[best_model][0][0], lbfgs_model_parameters[best_model][0][1], lbfgs_model_parameters[best_model][0][2]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ERPE394A",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}