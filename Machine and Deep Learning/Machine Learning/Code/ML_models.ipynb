{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu5X1t0ac4pn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import missingno as msno\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.cluster.hierarchy as shc\n",
        "from Util1 import DataManipulation\n",
        "from matplotlib import colors\n",
        "import matplotlib\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.pipeline      import make_pipeline\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KekeOpsgc4pp"
      },
      "source": [
        "# DATA MANIPULATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUoGbA7Jc4pr"
      },
      "outputs": [],
      "source": [
        "Data = pd.read_excel(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3\\Geophysical_Logs_Well_1.xlsx', sheet_name='Well_1')\n",
        "Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiDsBc7ec4ps"
      },
      "source": [
        "Remove Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S53AXke5c4ps"
      },
      "outputs": [],
      "source": [
        "m = DataManipulation(Data)\n",
        "m.VisualizeMissingData()\n",
        "m.MissingDataSummarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAtmXaTBc4ps"
      },
      "outputs": [],
      "source": [
        "Data_Manipulated = m.DropMissingData()\n",
        "m.VisualizeMissingData()\n",
        "m.MissingDataSummarizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mW6Vo_nc4ps"
      },
      "source": [
        "Remove Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cemyt5Acc4pt"
      },
      "outputs": [],
      "source": [
        "m.CrossPlot()\n",
        "m.BoxPlot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyc91PFMc4pt"
      },
      "outputs": [],
      "source": [
        "outlier_indexes = []\n",
        "for i in np.array(Data_Manipulated.columns):\n",
        "    if not i in ['Water Saturation (SW)']:\n",
        "        data_tukey = Data_Manipulated[i]\n",
        "        outliers = m.tukey_outliers(data_tukey, Data_Manipulated.index)[0]\n",
        "        idx = m.tukey_outliers(data_tukey, Data_Manipulated.index)[1]\n",
        "        outlier_indexes.append(idx)\n",
        "        print(i, outliers)\n",
        "        plt.hist(data_tukey, bins=50, color='gray', alpha=0.5)\n",
        "        plt.xlabel(i)\n",
        "        plt.ylabel('N')\n",
        "        plt.title(\"Outlier Detection\")\n",
        "        plt.scatter(outliers, [0 for j in range(len(outliers))], color='red')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYqC02-Lc4pt"
      },
      "source": [
        "Conbining the results from Tukey's outlier detection and Question 3, we can be sure that the numbers greater than or equal to 999 are outliers\n",
        "\n",
        "UPDATE: I decided to remove all the outliers that are dedected by TukeyFunction to improve regression prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpcfobhmc4pt"
      },
      "outputs": [],
      "source": [
        "outlier_indexes = np.hstack(outlier_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E67jJm58c4pt"
      },
      "outputs": [],
      "source": [
        "outlier_indexes =  np.unique(outlier_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2y4uR1ec4pu"
      },
      "outputs": [],
      "source": [
        "Data_Manipulated = Data_Manipulated.drop(outlier_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jml6w0FQc4pu"
      },
      "outputs": [],
      "source": [
        "Data_Manipulated = Data_Manipulated[Data_Manipulated['Water Saturation (SW)']<999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpdH91g7c4pu"
      },
      "outputs": [],
      "source": [
        "n= 0\n",
        "for i in Data_Manipulated['Water Saturation (SW)']:\n",
        "    if i == 1:\n",
        "        n+=1\n",
        "n # Number of SW = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuSpDE6-c4pu"
      },
      "outputs": [],
      "source": [
        "plt.plot(Data_Manipulated['Water Saturation (SW)'], 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUAEXiYPc4pu"
      },
      "outputs": [],
      "source": [
        "m = DataManipulation(Data_Manipulated)\n",
        "m.CrossPlot()\n",
        "m.BoxPlot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbf7FCe6c4pu"
      },
      "outputs": [],
      "source": [
        "m.MissingDataSummarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwZCIrYEc4pv"
      },
      "outputs": [],
      "source": [
        "m.DataAnalysisResults()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqBadpiMc4pv"
      },
      "source": [
        "# Question 1: Using the data (given as a separate file Geophysical_Logs_Well_1.xlsx), carry out the following analysis\n",
        "\n",
        "1. Use atleast two unsupervised learning techniques (Kmeans, DBscan, Hierarchical or Geometric means) to make appropriate number of clusters.\n",
        "2. Visualize and find the value of Silhouette coefficient.\n",
        "3. Improve the value of Silhouette coefficient by changing the number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww0UpIXdc4pv"
      },
      "outputs": [],
      "source": [
        "# Data scaling\n",
        "#INscaler=StandardScaler()\n",
        "#INDataScaled = INscaler.fit(Data_Manipulated.drop(['Water Saturation (SW)'], axis = 1))\n",
        "#OUTscaler=StandardScaler()\n",
        "#OUTDataScaled = OUTscaler.fit(np.array(Data_Manipulated['Water Saturation (SW)']).reshape((-1, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXm2i5cWc4pv"
      },
      "source": [
        "Saving Scaler to add pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AujeifJvc4pv"
      },
      "outputs": [],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "# save the model to a file\n",
        "#dump(INscaler, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\INscaler.joblib')\n",
        "#dump(OUTscaler, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\OUTscaler.joblib')\n",
        "# load the saved model from a file\n",
        "INscaler = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\INscaler.joblib')\n",
        "OUTscaler = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\OUTscaler.joblib')\n",
        "\n",
        "INDataScaled = INscaler.transform(Data_Manipulated.drop(['Water Saturation (SW)'], axis = 1))\n",
        "OUTDataScaled = OUTscaler.transform(np.array(Data_Manipulated['Water Saturation (SW)']).reshape((-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_CFIu4kc4pv"
      },
      "outputs": [],
      "source": [
        "class WellLog_Based_Predictor:\n",
        "    def __init__(self, clusterer_Data):\n",
        "        '''clusterer_Data = {'name' : 'KMeans', 'n_clusters': 5, 'random_state': 1000, 'init': 'k-means++', 'n_init':1000, 'max_iter':500} or {'name':'DBSCAN', 'eps': 0.9, 'min_samples': 24, 'metric':'euclidean'} or {'name' : 'GMM', 'n_clusters': 3}'''\n",
        "        self.clusterer_Data = clusterer_Data\n",
        "\n",
        "    def Clusterer(self, Data):\n",
        "        if self.clusterer_Data['name'] == 'KMeans':\n",
        "            n_clusters   = self.clusterer_Data['n_clusters']\n",
        "            random_state = self.clusterer_Data['random_state']\n",
        "            init         = self.clusterer_Data['init']\n",
        "            n_init       = self.clusterer_Data['n_init']\n",
        "            max_iter     = self.clusterer_Data['max_iter']\n",
        "            # Initialize the Clusterer\n",
        "            kmeans = KMeans(n_clusters= n_clusters, random_state= random_state, init=init, n_init= n_init, max_iter= max_iter)\n",
        "            # Fitting with inputs\n",
        "            kmeans = kmeans.fit(INDataScaled)\n",
        "            # print location of clusters learned by kmeans object\n",
        "            centers = kmeans.cluster_centers_\n",
        "            # Extract Labels\n",
        "            labels = kmeans.labels_\n",
        "            # inertia\n",
        "            inertia = kmeans.inertia_\n",
        "            return centers, labels, inertia\n",
        "\n",
        "        elif self.clusterer_Data['name'] == 'DBSCAN':\n",
        "            eps         = self.clusterer_Data['eps']\n",
        "            min_samples = self.clusterer_Data['min_samples']\n",
        "            metric      = self.clusterer_Data['metric']\n",
        "            # Initialize the Clusterer\n",
        "            Clustering= DBSCAN(eps= eps, min_samples=min_samples, metric=metric)\n",
        "            # Fit with input Data and Extract Labels\n",
        "            labels = Clustering.fit_predict(Data)\n",
        "            return labels\n",
        "\n",
        "        elif self.clusterer_Data['name'] == 'GMM':\n",
        "            n_clusters   = self.clusterer_Data['n_clusters']\n",
        "            # Initialize and Fit to Thge Data\n",
        "            gmm = GaussianMixture(n_components=int(n_clusters)).fit(Data)\n",
        "            # Predict Labels\n",
        "            labels = gmm.predict(Data)\n",
        "            return labels\n",
        "\n",
        "        else:\n",
        "            print('Entered Clusterer is Not Found')\n",
        "            print(\"Selections: DBSCAN KMeans GMM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eyomP8mc4pv"
      },
      "source": [
        "## Silhouiette Coefficient Calculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80B6Wuuic4pw"
      },
      "outputs": [],
      "source": [
        "def silhouette_coeff(labels, n, epsilon, data, clusterer = 'KMeans'):\n",
        "    n_clusters=np.unique(labels).shape[0]\n",
        "    silhouette_vals=silhouette_samples(data,labels,metric='euclidean')\n",
        "    y_ax_lower, y_ax_upper=0,0\n",
        "\n",
        "    yticks=[]\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    for i, c in enumerate (np.unique(labels)):\n",
        "        c_silhouette_vals= silhouette_vals[labels==c]\n",
        "        c_silhouette_vals.sort()\n",
        "        y_ax_upper += len(c_silhouette_vals)\n",
        "        color= cm.jet(float(i)/n_clusters)\n",
        "        plt.barh(range(y_ax_lower,y_ax_upper),c_silhouette_vals,height=1,edgecolor='none',color=color)\n",
        "        yticks.append((y_ax_lower+y_ax_upper)/2.)\n",
        "        y_ax_lower += len(c_silhouette_vals)\n",
        "    silhouette_avg=np.mean(silhouette_vals)\n",
        "    Plot1_text_s = \"The Silhouette Average for {} is {}\".format(clusterer, round(silhouette_avg,2))\n",
        "    plt.axvline(silhouette_avg,color=\"red\",linestyle=\"--\")\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.yticks(yticks, np.unique(labels) +1, fontsize=15)\n",
        "    plt.ylabel('Cluster', fontsize=15)\n",
        "    plt.xlabel('silhouette coefficient', fontsize=15)\n",
        "    plt.text(0.2, 0, Plot1_text_s, fontsize = 20, color =\"white\", bbox = {\"facecolor\": 'green'})\n",
        "    plt.show(False)\n",
        "    if clusterer == 'KMeans':\n",
        "        plt.savefig(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\silhcoef\\silhouette_coefficient {} #clusters  = {}.jpg'.format(clusterer, n), dpi=300)\n",
        "    elif clusterer == 'DBSCAN':\n",
        "        plt.savefig(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\silhcoef\\silhouette_coefficient {} epsilon  = {}.jpg'.format(clusterer, epsilon), dpi=300)\n",
        "    elif clusterer =='GMM':\n",
        "        plt.savefig(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\silhcoef\\silhouette_coefficient {} #clusters  = {}.jpg'.format(clusterer, n), dpi=300)\n",
        "    else:\n",
        "        print('Clusterer is Not Found')\n",
        "\n",
        "    return round(silhouette_avg,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvjJqwp5c4pw"
      },
      "source": [
        "## DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjI8XklOc4pw"
      },
      "source": [
        "### Finding Optimum Epsilon value for DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT7ewqLPc4pw"
      },
      "source": [
        "#### First,  Nearest Neigbour will be used to have sense about the epsilon value, which is around 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vjy6KhTc4pw"
      },
      "outputs": [],
      "source": [
        "Neighbors = NearestNeighbors(n_neighbors = 24) # 24 neighbors( Dimension  = 12, neigh = Dimension*2)\n",
        "nbrs = Neighbors.fit(DataScaled)\n",
        "distances, indices = nbrs.kneighbors(DataScaled)\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "distances = np.sort(distances, axis=0)\n",
        "for i in range(24):\n",
        "    plt.plot(distances[:, i])\n",
        "plt.title('Finding the optimum epsilon', fontsize = 15)\n",
        "plt.xlabel('Samples sorted by distance', fontsize = 15)\n",
        "plt.ylabel('NN distance', fontsize = 15)\n",
        "plt.axhline(y=0.9, color='k', linestyle='--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDfS2lDgc4pw"
      },
      "source": [
        "#### Now we know the closest interval for epsilon value which I will use [0.5, 2.5] to calculate sensitivity of Silhouette Coefficient respect to Epsilon Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVYnqXjmc4pw"
      },
      "source": [
        "#### Implementing DBSCSAN on Scaled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNfvAdK8c4pw"
      },
      "outputs": [],
      "source": [
        "silhouette_avg_list_DBSCAN = []\n",
        "n_unique_labels = []\n",
        "for i in np.linspace(0.5, 2.5, 20):\n",
        "    DBSCANclusterer_Data = {'name':'DBSCAN', 'eps': i, 'min_samples': 24, 'metric':'euclidean'}\n",
        "    mm = WellLog_Based_Predictor(DBSCANclusterer_Data)\n",
        "    DB_Labels = mm.Clusterer(INDataScaled)\n",
        "    silhouette_avg_list_DBSCAN.append(silhouette_coeff(DB_Labels, None, i, INDataScaled,'DBSCAN'))\n",
        "    n_unique_labels.append(np.unique(DB_Labels).shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-cObnkHc4px"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "eps = np.linspace(0.5, 2.5, 20)\n",
        "plt.plot(eps, silhouette_avg_list_DBSCAN, 'r-o')\n",
        "plt.xlabel('Epsilon', fontsize=18-2)\n",
        "plt.ylabel('Silhouette Coefficient', fontsize=18)\n",
        "plt.xticks(fontsize=18-2)\n",
        "plt.yticks(fontsize=18-2)\n",
        "for i, txt in enumerate(n_unique_labels):\n",
        "    plt.text(eps[i], silhouette_avg_list_DBSCAN[i], txt, fontsize = 12)\n",
        "plt.savefig('(DBSCAN) Silhouette Coefficient_vs_EPS_VS_#clusts.jpg', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSp3Sjh0c4px"
      },
      "source": [
        "## KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pde6J_qc4px"
      },
      "source": [
        "### Choosing optimum number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wczoXa3Tc4px"
      },
      "source": [
        "#### Again to have sense about the number of optimum clusters for KMeans I have used Dendrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpvdsD8Rc4px"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10), dpi = 300)\n",
        "dendogram = shc.dendrogram(shc.linkage(INDataScaled, method='ward'))\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.savefig(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3\\dendogram.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1pSTDYYc4px"
      },
      "source": [
        "#### Based on the Dendrogram, we cannot say exact cluster numbers ( Maybe 6-7), That is why I will calculate sensitivity of Silhoette Coefficient and Sum of Squared Errors respect to Number of Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRqaoGdSc4p2"
      },
      "outputs": [],
      "source": [
        "ERROR=[]\n",
        "KMeans_silhouette_avg_list = []\n",
        "for i in np.linspace(3, 20, 18):\n",
        "    KMeansclusterer_Data = {'name' : 'KMeans', 'n_clusters': int(i), 'random_state': 1000, 'init': 'k-means++', 'n_init':1000, 'max_iter':500}\n",
        "    mmm = WellLog_Based_Predictor(KMeansclusterer_Data)\n",
        "    centers, KM_Labels, inertia = mmm.Clusterer(DataScaled)\n",
        "    KMeans_silhouette_avg_list.append(silhouette_coeff(KM_Labels, i, None, DataScaled,'KMeans'))\n",
        "    ERROR.append(inertia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oGdxfdFc4p3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(np.linspace(3, 20, 18), ERROR, 'r-o')\n",
        "plt.xlabel('Number of Clusters', fontsize=18-2)\n",
        "plt.ylabel('Within Cluster Sum of Squared Errors', fontsize=18)\n",
        "plt.xticks(np.linspace(1,21, 21), fontsize=18-2)\n",
        "plt.yticks(fontsize=18-2)\n",
        "plt.savefig('KMeans_SumOfSqErr_#clusters.jpg', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdEwQ6gCc4p3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(np.linspace(3, 20, 18), KMeans_silhouette_avg_list, 'r-o')\n",
        "plt.xlabel('Number of Clusters', fontsize=18-2)\n",
        "plt.ylabel('Silhouette Coefficient', fontsize=18)\n",
        "plt.xticks(np.linspace(1,21, 21), fontsize=18-2)\n",
        "plt.yticks(fontsize=18-2)\n",
        "plt.savefig('(KMeans)Silhouette Coefficient_vs_#clusters.jpg', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqFc5O5Lc4p3"
      },
      "source": [
        "## GMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MdubM6fc4p3"
      },
      "source": [
        "### To be more sure, I decided to use GMM ( All procedure is the same as KMeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRu5uxQ8c4p3"
      },
      "outputs": [],
      "source": [
        "GMM_silhouette_avg_list = []\n",
        "for i in np.linspace(3, 20, 18):\n",
        "    GMMclusterer_Data = {'name' : 'GMM', 'n_clusters': int(i)}\n",
        "    mmmm = WellLog_Based_Predictor(GMMclusterer_Data)\n",
        "    GMM_Labels = mmmm.Clusterer(INDataScaled)\n",
        "    GMM_silhouette_avg_list.append(silhouette_coeff(GMM_Labels, i, None, INDataScaled,'GMM'))\n",
        "    ERROR.append(inertia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dfbZsnYc4p4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(np.linspace(3, 20, 18), GMM_silhouette_avg_list, 'r-o')\n",
        "plt.xlabel('Number of Clusters', fontsize=18-2)\n",
        "plt.ylabel('Silhouette Coefficient', fontsize=18)\n",
        "plt.xticks(np.linspace(1,21, 21), fontsize=18-2)\n",
        "plt.yticks(fontsize=18-2)\n",
        "plt.savefig('(GMM)Silhouette Coefficient_vs_#clusters.jpg', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbXeQHmic4p4"
      },
      "source": [
        "## Summary\n",
        "\n",
        "DBSCAN: Based on the Elbow method fron NearestNeighbour  0.9  with 7 clusters has been chosen as appropriate epsilon value. In silhouette results 0.9 epsilon corresponds to 0.2 (it is lower than 0.2). But later we can see that it increases. The maximum silhoette walue we got for DBSCAN is 0.6. However when we analyse silhouette graph we can see that data has been clustered in 2 clusters with about 1 to 9 ratio which does not haver any meaning\n",
        "\n",
        "KMeans: From the elbow method if we assume that cluster values are in the interval from 3 to 11, we got highest silhouette coeff 0.33 for 3. But the size of clusters are relatively high thats why in my opinion the cluster value which satisfies both elbow value and Silhoutte coeff is 7.\n",
        "\n",
        "GMM: Again same as KMeans we got highest silhouette coeff 0.3 for 3\n",
        "\n",
        "Eventually,  KMeans with n_clusters = 7\n",
        "\n",
        "UPDATE: Based on the previous results I decided to decrease the number of clusters to 3. The main reason is while training the regression models due to less number of data points in some clusters models cannot predict well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hheo_1Uc4p4"
      },
      "source": [
        "### Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8204kzYc4p4"
      },
      "outputs": [],
      "source": [
        "DataScaled_df = pd.DataFrame(INDataScaled, columns=Data.columns[0:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Muus6sQrc4p4"
      },
      "outputs": [],
      "source": [
        "DataScaled_df[\"Water Saturation (SW)\"] = OUTDataScaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lL_tLU8c4p4"
      },
      "outputs": [],
      "source": [
        "KMeansclusterer_Data = {'name' : 'KMeans', 'n_clusters': int(3), 'random_state': 1000, 'init': 'k-means++', 'n_init':1000, 'max_iter':500}\n",
        "mmm = WellLog_Based_Predictor(KMeansclusterer_Data)\n",
        "centers, KM_Labels, inertia = mmm.Clusterer(INDataScaled) # We have to cluster our data based on first 11 input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls8JNYCDc4p4"
      },
      "outputs": [],
      "source": [
        "KM_DataScaled = DataScaled_df.copy()\n",
        "KM_Data_Manipulated = Data_Manipulated.copy()\n",
        "KM_DataScaled['Labels'] = KM_Labels\n",
        "KM_Data_Manipulated['Labels'] = KM_Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MROsOSfLc4p4"
      },
      "outputs": [],
      "source": [
        "KM_DataScaled.to_csv(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\KM_DataScaled.csv', index=False)\n",
        "KM_Data_Manipulated.to_csv(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\KM_Data_Manipulated.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rex65QAGc4p5"
      },
      "source": [
        "# Question 2: Use atleast two supervised classification techniques (Random Forest, Gradient Boosting or KNN) to classify the dataset based on the appropriate number of clusters determined in Question 1. Use cluster as labels to train classification algorithm. with relevant well logs as inputs\n",
        "\n",
        "Provide Confusion matrix, recall, precision and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESQMhsFKc4p5"
      },
      "outputs": [],
      "source": [
        "KM_DataScaled = pd.read_csv(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\KM_DataScaled.csv')\n",
        "KM_DataScaled_TRAIN, KM_DataScaled_TEST = train_test_split(KM_DataScaled, train_size=0.95, random_state=42)\n",
        "Inputs  = KM_DataScaled_TRAIN.drop([\"Water Saturation (SW)\", 'Labels'],axis=1) # droping output variable from the input dataset\n",
        "Outputs = KM_DataScaled_TRAIN['Labels'] # Assigning output variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(Inputs, Outputs, test_size=0.30, random_state=42)\n",
        "\n",
        "KM_Data_Manipulated = pd.read_csv(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\KM_Data_Manipulated.csv')\n",
        "KM_Data_Manipulated_TRAIN, KM_Data_Manipulated_TEST = train_test_split(KM_Data_Manipulated, train_size=0.95, random_state=42)\n",
        "Inputs_NotScaled  = KM_Data_Manipulated_TRAIN.drop(['Labels'],axis=1) # droping output variable from the input dataset\n",
        "Outputs_NotScaled = KM_Data_Manipulated_TRAIN['Labels'] # Assigning output variable\n",
        "X_train_NotScaled, X_test_NotScaled, y_train_NotScaled, y_test_NotScaled = train_test_split(Inputs_NotScaled, Outputs_NotScaled, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKvmndPtc4p5"
      },
      "outputs": [],
      "source": [
        "def ClassificationResults(y_pred, y_test, classifier):\n",
        "    ConfMat = confusion_matrix(y_test, y_pred)\n",
        "    print(' Results for {} classifier'.format(classifier))\n",
        "    print('Accuracy Score:',accuracy_score(y_test, y_pred))\n",
        "    print('Confusion Matrix:')\n",
        "    print(ConfMat)\n",
        "    print('Classification Report')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    sns.heatmap(ConfMat, center=True, annot=True, cmap='turbo', linewidths=3, linecolor='black', fmt = \".1f\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_MCBkz3c4p5"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQM0kKGc4p5"
      },
      "source": [
        "Initalize and Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qPJiuqlc4p6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "RF = RandomForestClassifier(n_estimators=5000, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=2).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FToiA8Vnc4p6"
      },
      "source": [
        "Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqQBsM_3c4p6"
      },
      "outputs": [],
      "source": [
        "# save the model to a file\n",
        "dump(RF, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\RF_clssfr.joblib')\n",
        "\n",
        "# load the saved model from a file\n",
        "RF = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\RF_clssfr.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKwQyIPjc4p6"
      },
      "source": [
        "Predict and Visualzie Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K6MPRZRc4p6"
      },
      "outputs": [],
      "source": [
        "y_pred = RF.predict(X_test)\n",
        "ClassificationResults(y_pred, y_test, 'Random Forest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q342-P7ec4p6"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLZtWfI0c4p6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "GB =GradientBoostingClassifier(learning_rate=0.1, n_estimators=2000, criterion='friedman_mse',min_samples_split=2, min_samples_leaf=1, max_depth=3,max_features=None).fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3joaFREc4p6"
      },
      "source": [
        "Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK5IE7V5c4p6"
      },
      "outputs": [],
      "source": [
        "# save the model to a file\n",
        "dump(GB, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\GB_clssfr.joblib')\n",
        "\n",
        "# load the saved model from a file\n",
        "GB = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\GB_clssfr.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbFgYev9c4p7"
      },
      "source": [
        "Predict and Visualzie Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ddpk74pc4p7"
      },
      "outputs": [],
      "source": [
        "y_pred = GB.predict(X_test)\n",
        "ClassificationResults(y_pred, y_test, 'Gradient Boosting')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSqX600Yc4p7"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Both classifier has made good results with the Accuracy= 98%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqC-cIYYc4p7"
      },
      "outputs": [],
      "source": [
        "pipe_RF = make_pipeline(INscaler, RF)\n",
        "pipe_GB = make_pipeline(INscaler, GB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xttdqFHc4p7"
      },
      "source": [
        "# Question 3: Train atleast two supervised machine learning regression techniques (RF, GB, KNN, DT, or any other) on each cluster separately to predict the water saturation as label with relevant well logs as inputs.\n",
        "\n",
        "Provide training testing crossplots, RMSE, and R2 score for each model corresponds to each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFqrBJRmc4p7"
      },
      "outputs": [],
      "source": [
        "def data_cluster_masking(data, n_clusters, target):\n",
        "    mask_dict = {}\n",
        "    for i in range(n_clusters):\n",
        "        mask_dict[i] = data[target] == i\n",
        "    return mask_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TJhpCpwc4p7"
      },
      "outputs": [],
      "source": [
        "Inputs  = np.array(KM_Data_Manipulated_TRAIN.drop(['Water Saturation (SW)', 'Labels'],axis=1))     # droping output variable from the input dataset\n",
        "Outputs = np.array(KM_Data_Manipulated_TRAIN['Water Saturation (SW)']).reshape((-1,1))   # Assigning output variable\n",
        "\n",
        "# Adding Scaler module for each inputs and Outputs for inverse transformation\n",
        "\n",
        "mlp_In_scaler = StandardScaler()\n",
        "mlp_In_scaler.fit(Inputs)\n",
        "dump(mlp_In_scaler, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\mlp_In_scaler.joblib')\n",
        "mlp_Out_scaler = StandardScaler()\n",
        "mlp_Out_scaler.fit(Outputs)\n",
        "dump(mlp_Out_scaler, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\mlp_Out_scaler.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mvkPWmZc4p7"
      },
      "outputs": [],
      "source": [
        "mlp_In_scaler = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\mlp_In_scaler.joblib')\n",
        "Inputs_scaled = mlp_In_scaler.transform(Inputs)\n",
        "\n",
        "mlp_Out_scaler = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\mlp_Out_scaler.joblib')\n",
        "Outputs_scaled = mlp_Out_scaler.transform(Outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaDqMwS1c4p7"
      },
      "outputs": [],
      "source": [
        "Inputs_scaled_splitted = {}\n",
        "def data_cluster_splitting(Inputs, Outputs, data, n_clusters, target):\n",
        "    dict_splitted_Input_data = {}\n",
        "    dict_splitted_Output_data = {}\n",
        "    masks = data_cluster_masking(data, n_clusters, target)\n",
        "    for i in range (n_clusters):\n",
        "        dict_splitted_Input_data[i] = Inputs[masks[i]]\n",
        "        dict_splitted_Output_data[i] = Outputs[masks[i]]\n",
        "    return dict_splitted_Input_data, dict_splitted_Output_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51DKAy7Lc4p8"
      },
      "outputs": [],
      "source": [
        "def train_test_split_clustered_data(In, Out, n_clusters):\n",
        "    dic ={}\n",
        "    for i in range(n_clusters):\n",
        "        dic[i] = train_test_split(In[i], Out[i], test_size=0.30, random_state=42)\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH2mufsYc4p8"
      },
      "outputs": [],
      "source": [
        "IN_data, OUT_data = data_cluster_splitting(Inputs, Outputs, KM_Data_Manipulated_TRAIN, 3, target = 'Labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjzVz_mYc4p8"
      },
      "outputs": [],
      "source": [
        "train_test_splitted_data = train_test_split_clustered_data(IN_data, OUT_data, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwT5cM6Nc4p8"
      },
      "outputs": [],
      "source": [
        "def R2_pairity_RMSE(cluster_name,model_name,y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes):\n",
        "    \"\"\"R2 score RMSE and pairity plot calculation\"\"\"\n",
        "    # Get prediciton score in terms of R2\n",
        "    r2_test = r2_score(y_test_true, y_test_pred)\n",
        "    r2_train = r2_score(y_train_true, y_train_pred)\n",
        "\n",
        "    # Plot parity plots\n",
        "    fs = 18\n",
        "    plt.figure(figsize=(10, 25))\n",
        "\n",
        "    plt.suptitle(f'$SW$ Parity Plot', fontsize=fs)\n",
        "    plt.subplot(3, 1, 1)\n",
        "\n",
        "    plt.plot(y_test_true, y_test_pred, 'ro', label=f'Test: R2 = {r2_test: .3f}')\n",
        "    plt.plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'k-.')\n",
        "    plt.ylabel('Prediction, SW', fontsize=fs)\n",
        "\n",
        "    plt.legend(fontsize=fs,edgecolor='black')\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(y_train_true, y_train_pred, 'b^', label=f'Train: R2 = {r2_train: .3f}')\n",
        "    plt.plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'k-.')\n",
        "\n",
        "    plt.xlabel('True, SW', fontsize=fs)\n",
        "    plt.ylabel('Prediction, SW', fontsize=fs)\n",
        "    plt.legend(fontsize=fs, edgecolor='black')\n",
        "\n",
        "    plt.subplot(3, 1, 3)\n",
        "    train_RMSEmean = np.mean(train_RMSEscores, axis=1)\n",
        "    test_RMSEmean = np.mean(test_RMSEscores, axis=1)\n",
        "    plt.plot(train_sample_sizes, -train_RMSEmean, color=\"k\",label=\"Training RMSE score\")\n",
        "    plt.plot(train_sample_sizes, -test_RMSEmean,'--', color=\"k\",label=\"Cross-validation RMSE score\")\n",
        "    plt.title(\"Learning Curve for BHP Data Set with DT\")\n",
        "    plt.xlabel(\"Training Set Sample Size\"),\n",
        "    plt.ylabel(\"RMSE Score\"),\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Pairity Plots\\R2_parity_RMSE_plots_SW_{}_{}.png'.format(cluster_name,model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJXmxSKNc4p8"
      },
      "source": [
        "## Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrmtZFYmc4p8"
      },
      "outputs": [],
      "source": [
        "def estimator_sensitivity_RF(cluster_name, train_test_splitted_data, estimator_list):\n",
        "    \"\"\" this function is used to find optimal number of estimators for Random Forest Regressor\"\"\"\n",
        "    seed=42\n",
        "    n = cluster_name\n",
        "    np.random.seed(seed)\n",
        "    X_train_scaled = train_test_splitted_data[n][0]\n",
        "    y_train_scaled = train_test_splitted_data[n][2]\n",
        "    X_test_scaled  = train_test_splitted_data[n][1]\n",
        "    y_test_scaled  = train_test_splitted_data[n][3]\n",
        "    lstr2 = []\n",
        "    for i in estimator_list:\n",
        "        model    = RandomForestRegressor(n_estimators=int(i), max_depth=115,\n",
        "                                        min_samples_split=5,\n",
        "                                        random_state=42)\n",
        "        model.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "        y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "        y_test_pred = mlp_Out_scaler.inverse_transform(model.predict(X_test_scaled).reshape(-1, 1))\n",
        "        lstr2.append(r2_score(y_test_true, y_test_pred))\n",
        "    return lstr2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMV7ARbc4p8"
      },
      "source": [
        "### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9b_I296c4p9"
      },
      "outputs": [],
      "source": [
        "n = 1\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yup4Tc6Uc4p9"
      },
      "outputs": [],
      "source": [
        "cluster_name = 0\n",
        "estimator_list = np.linspace(65, 100, 36)\n",
        "lst = estimator_sensitivity_RF(cluster_name, train_test_splitted_data, estimator_list)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.linspace(65, 100, 36), lst, 'r-o')\n",
        "plt.xticks(np.linspace(65, 100, 36))\n",
        "plt.grid()\n",
        "plt.xlabel('# ESTIMATOR')\n",
        "plt.ylabel('R2 SCORE FOR TEST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ERN0X3c4p9"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "n = 0\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]\n",
        "\n",
        "model_RFregr_0    = RandomForestRegressor(n_estimators=98, max_depth=115,\n",
        "                                  min_samples_split=5,\n",
        "                                  random_state=42)\n",
        "dump(model_RFregr_0, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_RFregr_0.joblib')\n",
        "model_RFregr_0 = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_RFregr_0.joblib')\n",
        "model_RFregr_0.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "y_train_true = mlp_Out_scaler.inverse_transform(y_train_scaled)\n",
        "y_test_pred = mlp_Out_scaler.inverse_transform(model_RFregr_0.predict(X_test_scaled).reshape(-1, 1))\n",
        "y_train_pred = mlp_Out_scaler.inverse_transform(model_RFregr_0.predict(X_train_scaled).reshape(-1, 1))\n",
        "\n",
        "train_sample_sizes, train_RMSEscores,test_RMSEscores = learning_curve(model_RFregr_0,IN_data[n],OUT_data[n].flatten(),cv=10,\n",
        "                                                                    scoring='neg_root_mean_squared_error',\n",
        "                                                                    train_sizes=np.linspace(0.1,1.0,5),\n",
        "                                                                    random_state=42)\n",
        "\n",
        "R2_pairity_RMSE(n,\"RF_regr\",y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmU3tTXac4p9"
      },
      "source": [
        "### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbQRhmqhc4p9"
      },
      "outputs": [],
      "source": [
        "cluster_name = 1\n",
        "estimator_list = np.linspace(65, 100, 36)\n",
        "lst = estimator_sensitivity_RF(cluster_name, train_test_splitted_data, estimator_list)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.linspace(65, 100, 36), lst, 'r-o')\n",
        "plt.xticks(np.linspace(65, 100, 36))\n",
        "plt.grid()\n",
        "plt.xlabel('# ESTIMATOR')\n",
        "plt.ylabel('R2 SCORE FOR TEST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSEjXJpkc4p9"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "n = 1\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]\n",
        "\n",
        "model_RFregr_1    = RandomForestRegressor(n_estimators=65, max_depth=115,\n",
        "                                  min_samples_split=5,\n",
        "                                  random_state=42)\n",
        "dump(model_RFregr_1, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_RFregr_1.joblib')\n",
        "model_RFregr_1 = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_RFregr_1.joblib')\n",
        "model_RFregr_1.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "y_train_true = mlp_Out_scaler.inverse_transform(y_train_scaled)\n",
        "y_test_pred = mlp_Out_scaler.inverse_transform(model_RFregr_1.predict(X_test_scaled).reshape(-1, 1))\n",
        "y_train_pred = mlp_Out_scaler.inverse_transform(model_RFregr_1.predict(X_train_scaled).reshape(-1, 1))\n",
        "\n",
        "train_sample_sizes, train_RMSEscores,test_RMSEscores = learning_curve(model_RFregr_1,IN_data[n],OUT_data[n].flatten(),cv=10,\n",
        "                                                                    scoring='neg_root_mean_squared_error',\n",
        "                                                                    train_sizes=np.linspace(0.1,1.0,5),\n",
        "                                                                    random_state=42)\n",
        "\n",
        "R2_pairity_RMSE(n,\"RF_regr\",y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVLysm8wc4p-"
      },
      "source": [
        "### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajJx0dvYc4p-"
      },
      "outputs": [],
      "source": [
        "cluster_name = 2\n",
        "estimator_list = np.linspace(65, 130, 66)\n",
        "lst = estimator_sensitivity_RF(cluster_name, train_test_splitted_data, estimator_list)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(estimator_list, lst, 'r-o')\n",
        "plt.xticks(estimator_list)\n",
        "plt.grid()\n",
        "plt.xlabel('# ESTIMATOR')\n",
        "plt.ylabel('R2 SCORE FOR TEST')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7A85KAMc4p-"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUfawwIFc4p-"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "n = 2\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]\n",
        "\n",
        "model_RFregr_2    = RandomForestRegressor(n_estimators=108, max_depth=115,\n",
        "                                  min_samples_split=5,\n",
        "                                  random_state=42)\n",
        "dump(model_RFregr_2, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_RFregr_2.joblib')\n",
        "model_RFregr_2 = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_RFregr_2.joblib')\n",
        "model_RFregr_2.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "y_train_true = mlp_Out_scaler.inverse_transform(y_train_scaled)\n",
        "y_test_pred = mlp_Out_scaler.inverse_transform(model_RFregr_2.predict(X_test_scaled).reshape(-1, 1))\n",
        "y_train_pred = mlp_Out_scaler.inverse_transform(model_RFregr_2.predict(X_train_scaled).reshape(-1, 1))\n",
        "\n",
        "train_sample_sizes, train_RMSEscores,test_RMSEscores = learning_curve(model_RFregr_2,IN_data[n],OUT_data[n].flatten(),cv=10,\n",
        "                                                                    scoring='neg_root_mean_squared_error',\n",
        "                                                                    train_sizes=np.linspace(0.1,1.0,5),\n",
        "                                                                    random_state=42)\n",
        "\n",
        "R2_pairity_RMSE(n,\"RF_regr\",y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHNpqPtlc4p-"
      },
      "source": [
        "### It is observed from the R2 scores of 7 models, most of the model have relatively satisfactory R2 scores. Less R2 scores is the result of less number of train data for the cluster, in other words model cannot be trained well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrE_5EiKc4p-"
      },
      "source": [
        "## Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slwzC2onc4p-"
      },
      "outputs": [],
      "source": [
        "def estimator_sensitivity_GB(cluster_name, train_test_splitted_data, estimator_list):\n",
        "    \"\"\" this function is used to find optimal number of estimators for Gradient Boosting Regressor\"\"\"\n",
        "    seed=42\n",
        "    n = cluster_name\n",
        "    np.random.seed(seed)\n",
        "    X_train_scaled = train_test_splitted_data[n][0]\n",
        "    y_train_scaled = train_test_splitted_data[n][2]\n",
        "    X_test_scaled  = train_test_splitted_data[n][1]\n",
        "    y_test_scaled  = train_test_splitted_data[n][3]\n",
        "    lstr2 = []\n",
        "    for i in estimator_list:\n",
        "        model = GradientBoostingRegressor(n_estimators=int(i), learning_rate=0.01, max_depth=5,\n",
        "                                            random_state=48,\n",
        "                                            loss='squared_error')\n",
        "        model.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "        y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "        y_test_pred = mlp_Out_scaler.inverse_transform(model.predict(X_test_scaled).reshape(-1, 1))\n",
        "        lstr2.append(r2_score(y_test_true, y_test_pred))\n",
        "    return lstr2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijA8_n8ec4p-"
      },
      "source": [
        "### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rFC_3Xwc4p-"
      },
      "outputs": [],
      "source": [
        "cluster_name = 0\n",
        "estimator_list = np.linspace(600, 900, 26)\n",
        "lst = estimator_sensitivity_GB(cluster_name, train_test_splitted_data, estimator_list)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(estimator_list, lst, 'r-o')\n",
        "plt.xticks(estimator_list)\n",
        "plt.grid()\n",
        "plt.xlabel('# ESTIMATOR')\n",
        "plt.ylabel('R2 SCORE FOR TEST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4brSevAc4p_"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "n = 0\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]\n",
        "\n",
        "model_GBregr_0 = GradientBoostingRegressor(n_estimators=900, learning_rate=0.01, max_depth=5,\n",
        "                                    random_state=48,\n",
        "                                    loss='squared_error')\n",
        "dump(model_GBregr_0, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_GBregr_0.joblib')\n",
        "model_GBregr_0 = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_GBregr_0.joblib')\n",
        "model_GBregr_0.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "y_train_true = mlp_Out_scaler.inverse_transform(y_train_scaled)\n",
        "y_test_pred = mlp_Out_scaler.inverse_transform(model_GBregr_0.predict(X_test_scaled).reshape(-1, 1))\n",
        "y_train_pred = mlp_Out_scaler.inverse_transform(model_GBregr_0.predict(X_train_scaled).reshape(-1, 1))\n",
        "\n",
        "train_sample_sizes, train_RMSEscores,test_RMSEscores = learning_curve(model_GBregr_0,IN_data[n],OUT_data[n].flatten(),cv=10,\n",
        "                                                                    scoring='neg_root_mean_squared_error',\n",
        "                                                                    train_sizes=np.linspace(0.1,1.0,5),\n",
        "                                                                    random_state=42)\n",
        "\n",
        "R2_pairity_RMSE(n,\"GB_regr\",y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcbSsUUCc4p_"
      },
      "source": [
        "### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCqmE4Spc4p_"
      },
      "outputs": [],
      "source": [
        "cluster_name = 1\n",
        "estimator_list = np.linspace(600, 900, 26)\n",
        "lst = estimator_sensitivity_RF(cluster_name, train_test_splitted_data, estimator_list)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(estimator_list, lst, 'r-o')\n",
        "plt.xticks(estimator_list)\n",
        "plt.grid()\n",
        "plt.xlabel('# ESTIMATOR')\n",
        "plt.ylabel('R2 SCORE FOR TEST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYzO4nOYc4p_"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "n = 1\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]\n",
        "\n",
        "model_GBregr_1    = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01, max_depth=5,\n",
        "                                    random_state=48,\n",
        "                                    loss='squared_error')\n",
        "dump(model_GBregr_1, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_GBregr_1.joblib')\n",
        "model_GBregr_1 = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_GBregr_1.joblib')\n",
        "model_GBregr_1.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "y_train_true = mlp_Out_scaler.inverse_transform(y_train_scaled)\n",
        "y_test_pred = mlp_Out_scaler.inverse_transform(model_GBregr_1.predict(X_test_scaled).reshape(-1, 1))\n",
        "y_train_pred = mlp_Out_scaler.inverse_transform(model_GBregr_1.predict(X_train_scaled).reshape(-1, 1))\n",
        "\n",
        "train_sample_sizes, train_RMSEscores,test_RMSEscores = learning_curve(model_GBregr_1,IN_data[n],OUT_data[n].flatten(),cv=10,\n",
        "                                                                    scoring='neg_root_mean_squared_error',\n",
        "                                                                    train_sizes=np.linspace(0.1,1.0,5),\n",
        "                                                                    random_state=42)\n",
        "\n",
        "R2_pairity_RMSE(n,\"GB_regr\",y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO3nyuvhc4p_"
      },
      "source": [
        "### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnYK2se5c4p_"
      },
      "outputs": [],
      "source": [
        "cluster_name = 2\n",
        "estimator_list = np.linspace(600, 900, 26)\n",
        "lst = estimator_sensitivity_RF(cluster_name, train_test_splitted_data, estimator_list)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(estimator_list, lst, 'r-o')\n",
        "plt.xticks(estimator_list)\n",
        "plt.grid()\n",
        "plt.xlabel('# ESTIMATOR')\n",
        "plt.ylabel('R2 SCORE FOR TEST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sviP0v-9c4p_"
      },
      "outputs": [],
      "source": [
        "seed=42\n",
        "np.random.seed(seed)\n",
        "n = 2\n",
        "X_train_scaled = train_test_splitted_data[n][0]\n",
        "y_train_scaled = train_test_splitted_data[n][2]\n",
        "X_test_scaled  = train_test_splitted_data[n][1]\n",
        "y_test_scaled  = train_test_splitted_data[n][3]\n",
        "\n",
        "model_GBregr_2    = GradientBoostingRegressor(n_estimators=672, learning_rate=0.01, max_depth=5,\n",
        "                                    random_state=48,\n",
        "                                    loss='squared_error')\n",
        "dump(model_GBregr_2, r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_GBregr_2.joblib')\n",
        "model_GBregr_2 = load(r'C:\\Users\\YUSIFOH\\ERPE 394A\\Assignment 3 copy\\Saved Models\\model_GBregr_2.joblib')\n",
        "model_GBregr_2.fit(X_train_scaled,y_train_scaled.flatten())\n",
        "\n",
        "y_test_true = mlp_Out_scaler.inverse_transform(y_test_scaled)\n",
        "y_train_true = mlp_Out_scaler.inverse_transform(y_train_scaled)\n",
        "y_test_pred = mlp_Out_scaler.inverse_transform(model_GBregr_2.predict(X_test_scaled).reshape(-1, 1))\n",
        "y_train_pred = mlp_Out_scaler.inverse_transform(model_GBregr_2.predict(X_train_scaled).reshape(-1, 1))\n",
        "\n",
        "train_sample_sizes, train_RMSEscores,test_RMSEscores = learning_curve(model_GBregr_2,IN_data[n],OUT_data[n].flatten(),cv=10,\n",
        "                                                                    scoring='neg_root_mean_squared_error',\n",
        "                                                                    train_sizes=np.linspace(0.1,1.0,5),\n",
        "                                                                    random_state=42)\n",
        "\n",
        "R2_pairity_RMSE(n,\"GB_regr\",y_train_true, y_train_pred, y_test_true, y_test_pred, train_RMSEscores, test_RMSEscores, train_sample_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbvlL9rMc4p_"
      },
      "source": [
        "# Question 4: Develop a python code that takes well logs as input and determine the relevant cluster and predict the water saturation as an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfIKvgX7c4qA"
      },
      "outputs": [],
      "source": [
        "def predictor( classifier, regressor, data):\n",
        "    if classifier == \"GB\":\n",
        "        pipe = make_pipeline(INscaler, GB)\n",
        "    elif classifier == \"RF\":\n",
        "        pipe = make_pipeline(INscaler, RF)\n",
        "\n",
        "    classes = pipe.predict(data)\n",
        "    data = np.array(data)\n",
        "    if regressor == \"RF\":\n",
        "        results = []\n",
        "        for i in range(len(classes)):\n",
        "            if classes[i] == 0:\n",
        "                pipe_r = make_pipeline(mlp_In_scaler, model_RFregr_0)\n",
        "                Sw = mlp_Out_scaler.inverse_transform((pipe_r.predict(data[i].reshape((1,-1)))).reshape((1,-1)))\n",
        "                results.append(Sw)\n",
        "            elif classes[i] == 1:\n",
        "                pipe_r = make_pipeline(mlp_In_scaler, model_RFregr_1)\n",
        "                Sw = mlp_Out_scaler.inverse_transform((pipe_r.predict(data[i].reshape((1,-1)))).reshape((1,-1)))\n",
        "                results.append(Sw)\n",
        "            elif classes[i] == 2:\n",
        "                pipe_r = make_pipeline(mlp_In_scaler, model_RFregr_2)\n",
        "                Sw = mlp_Out_scaler.inverse_transform((pipe_r.predict(data[i].reshape((1,-1)))).reshape((1,-1)))\n",
        "                results.append(Sw)\n",
        "\n",
        "    elif regressor == \"GB\":\n",
        "        results = []\n",
        "        for i in range(len(classes)):\n",
        "            if classes[i] == 0:\n",
        "                pipe_r = make_pipeline(mlp_In_scaler, model_GBregr_0)\n",
        "                Sw = mlp_Out_scaler.inverse_transform((pipe_r.predict(data[i].reshape((1,-1)))).reshape((1,-1)))\n",
        "                results.append(Sw)\n",
        "            elif classes[i] == 1:\n",
        "                pipe_r = make_pipeline(mlp_In_scaler, model_GBregr_1)\n",
        "                Sw = mlp_Out_scaler.inverse_transform((pipe_r.predict(data[i].reshape((1,-1)))).reshape((1,-1)))\n",
        "                results.append(Sw)\n",
        "            elif classes[i] == 2:\n",
        "                pipe_r = make_pipeline(mlp_In_scaler, model_GBregr_2)\n",
        "                Sw = mlp_Out_scaler.inverse_transform((pipe_r.predict(data[i].reshape((1,-1)))).reshape((1,-1)))\n",
        "                results.append(Sw)\n",
        "    return classes, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr4rLDn0c4qA"
      },
      "outputs": [],
      "source": [
        "user_input = KM_Data_Manipulated_TEST.drop(['Water Saturation (SW)', 'Labels'], axis = 1)  # user input\n",
        "classes_true = np.array(KM_Data_Manipulated_TEST['Labels'])\n",
        "Sw_true =np.array(KM_Data_Manipulated_TEST['Water Saturation (SW)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMCEs-WJc4qA"
      },
      "outputs": [],
      "source": [
        "class_pred = predictor( 'GB', \"GB\", user_input)[0]\n",
        "Sw_pred = predictor( 'GB', \"GB\", user_input)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQO3hA4Kc4qA"
      },
      "outputs": [],
      "source": [
        "true = 0\n",
        "for i in range(len(classes_true)):\n",
        "    if classes_true[i] == class_pred[i]:\n",
        "        true += 1\n",
        "acc_class = (true/len(classes_true)) * 100\n",
        "acc_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFNX19Qgc4qA"
      },
      "outputs": [],
      "source": [
        "acc_sw = np.average((Sw_true - Sw_pred)**2)\n",
        "acc_sw"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ERPE394A",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}